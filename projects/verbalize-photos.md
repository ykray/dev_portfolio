# [verbalize.photos](https://verbalize.photos)

![verbalize-photos-screenshot](/assets/verbalize-photos-ss.png)

The Internet is heavily visual—over _3 billion_ images are shared over the internet _daily_—most of which are completely hidden from users who can’t physically _see_ them. And developers don't do enough to address this area of accessibility—skimping on providing descriptive alt tags, aria-labels, and other means of supporting the accessibility of visual content. This project’s goal was to expand the experience of visual content by involving other senses in the absence of sight.

[verbalize.photos](https://verbalize.photos) was built in React as final submission for my _Human Computer Interaction_ course at NYU. It aims to improve the experience of the visual side of the internet for visually-impaired users. Users can simply paste a photo or a link to a photo, and a human-like, verbal description of the photo is generated using Computer Vision, Speech Synthesis, and Natural Language Processing, along with Braille transcription (for Refreshable Braille Displays), and (experimental) “ambience” audio; further enhancing the imaginative experience by involving another sense of perception. Supports _voice control_, _keyboard navigation_, and _multiple languages_ (English, French, Polish, Ukrainian, Russian).

## Skills

- TypeScript
- React
- Node.js
- AWS
- Azure
- Firebase
- MUI
- SCSS
