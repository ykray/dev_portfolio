# verbalize.photos

![verbalize-photos-screenshot](/assets/verbalize-photos-ss.png)

The Internet is heavily visual—over _3 billion_ images are shared over the internet _daily_—most of which are completely hidden from users who can’t physically _see_ them. And developers don't do enough to address this area of accessibility—skimping on providing descriptive alt tags, aria-labels, and other means of supporting the accessibility of visual content. This project’s goal was to expand the experience of visual content by involving other senses in the absence of sight.

**verbalize.photos** was built with React as the final project for my _Human Computer Interaction_ course at NYU. It aims to improve the experience of the visual side of the internet for visually-impaired users. Users can simply paste a photo or a link to a photo, and a human-like, verbal description of the photo is generated using Computer Vision, Speech Synthesis, and Natural Language Processing, along with Braille transcription (for Refreshable Braille Displays), and (experimental) “ambience” audio; further enhancing the imaginative experience by involving another sense of perception. Supports _voice control_, _keyboard navigation_, and _multiple languages_ (English, French, Polish, Ukrainian, Russian).

### Built with

`typescript` `javascript` `react` `node` `azure` `firebase` `mui` `scss`
